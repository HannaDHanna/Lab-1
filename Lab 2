
# Load data

 data1_surgery = read.csv("https://raw.githubusercontent.com/kekecsz/SIMM61-Course-materials/main/Home_assignment/surgery_data_1.csv")
 
 data2_surgery = read.csv("https://raw.githubusercontent.com/kekecsz/SIMM61-Course-materials/main/Home_assignment/surgery_data_2.csv")

 # Look at both data sets for overview
 
 view(data1_surgery) 

 view(data2_surgery)
 
 str(data1_surgery)
 
 str(data2_surgery)

# Load packages
 
 library(psych)
 library(tidyverse) 
 library(lme4) 
 library(lmerTest) 
 library(cAIC4) 
 library(r2glmm) 
 library(MuMIn) 
 library(blme) 
 
 
 # Inspect the data to catch missing data
 
 colSums(is.na(data1_surgery))
 
 colSums(is.na(data2_surgery))
 
 # No missing values or blanks, this goes for both data sets
 
 # Look at the variables of data 1
 
 view(data1_surgery$pain)
 
 class(data1_surgery$pain)
 
 view(data1_surgery$mindfulness)
 
 class(data1_surgery$mindfulness)
 
 view(data1_surgery$STAI_trait)
 
 class(data1_surgery$STAI_trait)
 
 view(data1_surgery$pain_cat)
 
 class(data1_surgery$pain_cat)
 
 unique(data1_surgery)
 
 unique(data2_surgery)
 # Get an overview of the data
 
 describe(data1_surgery)
 
 summary(data1_surgery)
 
 # The income variable is a bit strange, someone has a negative income, but I will not use this variable and therefore I don't fix it.
 
 # Visualizing the data
 
 # Visualizing of the Stai-trait (x) variable on pain (y)
 data1_surgery %>%
   ggplot() + aes(y = pain, x = STAI_trait) + geom_point(aes(color = STAI_trait),
                                                                 size = 4) + geom_smooth(method = "lm", se = F)

 
 int_plot = data1_surgery %>%
   ggplot() + aes(y = pain, x = STAI_trait) +
   geom_point(size = 4) + geom_smooth(method = "lm", se = F,
                                      fullrange = TRUE) 
 
 int_plot
 
 boxplot(data1_surgery$pain~ data1_surgery$pain_cat)
 
 # Visualizing the pain-cat (x) varible on pain(y)
 
 data1_surgery %>%
   ggplot() + aes(y = pain, x = pain_cat) + geom_point(aes(color = pain_cat),
                                                         size = 4) + geom_smooth(method = "lm", se = F)
 
 # Visualizing  the coritisol serum varible (x) on pain (y)
 
 data1_surgery %>%
   ggplot() + aes(y = pain, x = cortisol_serum) + geom_point(aes(color = pain),
                                                         size = 4) + geom_smooth(method = "lm", se = F)
 
 # Visualizing the age varible (x) on pain (y)
 data1_surgery %>%
   ggplot() + aes(y = pain, x = age) + geom_point(aes(color = pain),
                                                             size = 4) + geom_smooth(method = "lm", se = F)
 
 boxplot(data1_surgery$pain~ data1_surgery$sex)
 
 # Visualizing the sex varible (x) on pain (y)
 
 boxplot(data1_surgery$pain~ data1_surgery$sex)
 
 # When viewing the box plot of sex and pain I see two columns for women, females and women. I therefore re-code sex as numeric and then make women and female into 0 and male into 1
 
 # Re-code the sex value to numeric, and then make women and female (1 and 3 into 0) and male (2) into 1
 
 class(data1_surgery$sex)
 view(data1_surgery$sex)
 data1_surgery <- data1_surgery %>% 
   mutate(sex = factor(recode(sex, "male"= "0", "female"= "1", "woman"= "1")))

  # Doing it for data 2
 
 
 data2_surgery <- data2_surgery %>% 
    mutate(sex = factor(recode(sex, "male"= "0", "female"= "1")))
 
# 0   1 
 #98 102 
 
 # Females are now 1 and males are 0
 table(data2_surgery$sex)
 view(data2_surgery$sex)
 view(data1_surgery$sex)
 class(data2_surgery$sex)
 
# men   female 
# 103   97 
 
 # More visualization

 # Hospital as x and pain as y
 boxplot(data1_surgery$pain~ data1_surgery$hospital)
 
 # Mindfulness as x and pain as y 
 data1_surgery %>%
   ggplot() + aes(y = pain, x = mindfulness) + geom_point(aes(color = mindfulness),
                                                       size = 4) + geom_smooth(method = "lm", se = F)
 
 # Adding the hospital varible as group by to see difference in different hospitals, do they add variation to the pain levels and the effects of the predictor variable?
 
 slope_plot = data1_surgery %>%
   ggplot() + aes(y = pain, x = mindfulness, color = hospital) +
   geom_point(size = 4) + geom_smooth(method = "lm", se = F,
                                      fullrange = TRUE) + xlim(10, -0) + geom_hline(yintercept = 0) +
   geom_vline(xintercept = 0)
 slope_plot
 
 slope_plot2 = data1_surgery %>%
   ggplot() + aes(y = pain, x = STAI_trait, color = hospital) +
   geom_point(size = 4) + geom_smooth(method = "lm", se = F,
                                      fullrange = TRUE) + xlim(60,10) + geom_hline(yintercept = 0) +
geom_vline(xintercept = 0)
 
slope_plot2

# The vizualisation showed cortisol serum, age and pain_cat, mindfulness, stai to be influential the pain level

 
# Creating a Mixed model regression including with age, sex, STAI, pain catastrophizing, mindfulness, and serum cortisol as fixed effect predictors. 
 
mixed_model.1 = lmer(pain ~ STAI_trait+ pain_cat + age+ sex + mindfulness+ cortisol_serum  + (1 | hospital), data = data1_surgery)
 
summary(mixed_model.1)

# Calculating the variance explained

mean_model.TSS <- lm(pain ~ 1, data = data1_surgery) # null-model

# 1-(RSS/TSS) = R^2

RSS = sum((data2_surgery$pain - predicfordata2)^2)

    TSS = sum((data2_surgery$pain - predict(mean_model.TSS))^2)
    
R2 = 1-(RSS/TSS) #####  37,9. on this model
R2 

summary(mixed_model.1)

summary(mean_model.TSS)    

# Confidence intervals of the mixed model

confint(mixed_model.1) 

# Look at second data set

view(data2_surgery) 

describe(data2_surgery)
summary(data2_surgery)

colSums(is.na(data2_surgery))

### Making a mixed linear model with random intercept to compare to the model that has the same predictor but
mixed_model.55 = lmer(pain ~ cortisol_serum  + (1 | hospital), data = data2_surgery)

mixed_model.6 = lmer(pain ~ cortisol_serum  + (1 | hospital), data = data1_surgery)

# Conditional and mariginal R squared

r.squaredGLMM(mixed_model.1)

#   R2m           R2c
# 0.3852492, 38% 0.4632079, 46%

# Could also be extracted by 

modelsummary::modelsummary(mixed_model.1)


1-(RSS/TSS)

= R^2

unique(data2_surgery)

RSS2 = sum((data2_surgery$pain - predict(mixed_model.2))^2)

TSS2 = sum((data2_surgery$pain - predict(mean_model.TSS2))^2)

mean_model.TSS2 <- lm(pain ~ 1, data = data1_surgery)


##### using the predictions from data 1 to predict pain on data two.
predicfordata2<- predict(mixed_model.1, newdata = data2_surgery, allow.new.levels = TRUE)

RSS = sum((data2_surgery$pain - predicfordata2)^2)

TSS = sum((data2_surgery$pain - predict(mean_model.TSS))^2)

R2 = 1-(RSS/TSS) #####  37,9. on this model
R2 

##### Nu är det närmare marginal
view(data1_surgery)
view(data2_surgery)
R2.2 = 1-(RSS2/TSS2) 
R2.2 
R2

# Coriolis, mindfulness, age

mixed_model.3 = lmer(pain ~ cortisol_serum + (cortisol_serum|hospital), data = data1_surgery)
 summary(mixed_model.3)
 
 #### Doing a mixed model with a random slope but only adding the most influential predictor, cortisol serum, and allowing for random slope on the different hospitals, y is still pain.
 ### Using the blmer function to avoid getting the warning message of isSinglur on the model with random effects and random slope
 
 # installing package
 library(blme)
 
 
 # doing the random slope with blmer and an optimizer called Nelder Mead :) this is done because without you get the warning message of isSingular, by adding an optimizer and doing it with Bayesian model you reduce the risk of this
 mod_rnd_slope_opt = blmer(pain ~ cortisol_serum + (cortisol_serum |
                                                     hospital), control = lmerControl(optimizer = "Nelder_Mead"),
                          data = data1_surgery)
 
 
 summary(mod_rnd_slope_opt)
 
### Visualization 
 
# Save the slopes of the mixed model and the mixed model with the random slope
 
 pain_performance_slope = data1_surgery %>%
   mutate(pred_int = predict(mixed_model.6), pred_slope = predict(mod_rnd_slope_opt))
 
 
 summary(pain_performance_slope)
 
 ## Vizualisation of the mixed model with random slope
 pain_performance_slope %>%
   ggplot() + aes(y = pain, x = cortisol_serum, group = hospital) +
   geom_point(aes(color = hospital), size = 4) + geom_line(color = "red",
                                                        aes(y = pred_slope, x = cortisol_serum)) + facet_wrap(~hospital,
                                                                                                            ncol = 2)
 ##### Visualization with the mixed effects model
 
 pain_performance_slope %>%
   ggplot() + aes(y = pain, x = cortisol_serum, group = hospital) +
   geom_point(aes(color = hospital), size = 4) + geom_line(color = "red",
                                                        aes(y = pred_int, x = cortisol_serum)) + facet_wrap(~hospital,
                                                                                                     ncol = 2)

 ### Looking at confidence intervals 
 
 confint(mixed_model.1, level = 0.95)
 confint(mixed_model.1) # when you just ask it to compute the confint intervals it gives you the same vlaue as printing for 95%, therefor this must be the overall confidence interval
 
 #### Visualizing the coefficients of the model
 
 library(visreg)
 visreg(mixed_model.1, "cortisol_serum", # specifying x-variable to visualize
        ylab = "pain", 
        xlab = "cortisol_serum", 
        gg = TRUE, # creating a ggplot
        band = TRUE) + 
   theme_classic() + 
   ggtitle("Big model")
 
 modelsummary::modelsummary(mixed_model.1)
 
 modelsummary::modelsummary(mixed_model.2)
 
 # Looking for influential outliers
 library(influence.ME)
 
influence(mixed_model.1, obs = TRUE)
 
influence(mod_rnd_slope_opt, obs = TRUE)

influence(mod_rnd_slope_opt, obs = T)

data_influence<-influence(mod_rnd_slope_opt, group = "hospital", data= data1_surgery)

view(data_influence)

data_influence2<-influence(mod_rnd_slope_opt, group = "hospital", data= data1_surgery)$alt.fixed

view(data_influence2)

data_plot_inflience = as_tibble(data_influence2) %>%
   gather(colnames(data_influence2), value = coefficient, key = predictor)

data_plot_inflience %>%
   ggplot() + aes(x = 1, y = coefficient, group = predictor) +
   geom_violin() + geom_jitter(width = 0.2) + facet_wrap(~predictor,
                                                         scales = "free")
# No outlier or clustering found in the data, the plots indicate that some datapoints are a bit of but not alot and not many :)

 # Normality

library(lme4)
(mod_rnd_slope_opt)

data1_surgery %>%
   ggplot() + aes(sample= pain) + stat_qq()+ stat_qq_line() +
   facet_wrap(~hospital, scales = "free")


random_effects = as.data.frame(ranef(mod_rnd_slope_opt)[[1]])
names(random_effects) = c("intercept")

random_effects %>%
   ggplot() + aes(sample = intercept) + stat_qq() + stat_qq_line()

describe(random_effects$intercept)$skew
#0.209166

describe(random_effects$intercept)$kurtosis
 #-1.308577
# Normality looks fine too

# Linearity

plot(mod_rnd_slope_opt, arg = "pearson")
plot(mixed_model.1, arg = "pearson")

data1_surgery %>%
   ggplot() + aes(x = "cortisol serum", y = "pain")+ geom_point()
# Looked alright.

 # Homoscedasticity

plot(mod_rnd_slope_opt, arg = "pearson")
plot(mixed_model.1, arg = "pearson")
 
# No cone shape appeared in eighter one of the models.

# Multicollinearity
 
pairs.panels(data1_surgery[, c("pain", "cortisol_serum", "hospital")], col = "red", lm = T)

# seems fine. Normal distributed curves. 
modelsummary::modelsummary(mod_rnd_slope_opt)
### The end
